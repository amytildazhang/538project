---
title: "511 Project"
author: "Amy Zhang"
date: "November 24, 2015"
output: pdf_document
---

Nate Silver made waves in the 2008 election when he correctly forecasted the outcomes of 49 out of 50 of the states using statistics ______. I attempted to replicate his predictions for the 2012 election, using the methodology described on his blog [1]. 

Nate Silver's method is based on the idea that general election polls are related to the outcome of the election, and can be used to create a prediction of the election's result. If we view the election results as a random variable, we can use the polls to define an expected value for the election's outcome, and the uncertainty associated with the polls as the variance. 

A simple aggregated poll average would not be enough to create a prediction because not all polls are created equal--some are more accurate due to methodology, others due to sample size. Some polls may be from a long time ago, and therefore from a different political landscape and thus less applicable to the current political landscape. A good model should account for all of these factors in the data. A large part of Nate Silver's process is adjusting and weighting the polls for the election to account for these factors.

He first collects poll data and weights each poll based on its recency, sample size, and a rating that he creates for the pollster who conducted the poll, based on the pollster's historical accuracy in predicting elections. This way, more recent polls are given more weight, as are polls with larger sample sizes. 

He then adjusts the poll results so that they are comparable. For example, a certain pollster may have a liberal tendency, and on average give 2 percentage points more to the Democrat than other polls. This pollster's polls would then be adjusted 2 points toward the Republican party. This "house effects" difference is one of three factors Nate Silver considers when adjusting the poll results. The other two are whether the poll sampled from likely voters or registered voters and the overall political mood at the time the poll was taken.

After completing these two steps, I used the adjusted, weighted polling average to create a prediction for the outcome in each state. Due to time constraints, I did not get to include each state's political partisanship,  the nation's economy, or other factors such as each candidate's partisan index in my prediction, and I was not able to quantify the uncertainty associated with the polls. This heavily affects the interpretation of my results, compared to Nate Silver's projections, because although I have provided an estimate of the eleciton's outcome, I cannot say how good that estimate is.

To replicate Nate Silver's results, I used his dataset of pollster ratings, found at https://github.com/fivethirtyeight/data/tree/master/pollster-ratings, and a CSV file of general election polls from 2012 found on _____. 

#Weighted Polling Average
##Recency
Polls are weighted according to how recent they are, using an exponential decay function $exp{-\lambda t}$, where t is the number of days since the poll was conducted, and $\lambda$ is chosen so that the half-life of the poll is $14 + 0.2*\text{daysfrom}$, where $\text{daysfrom}$ is the number of days between the current day and the election. The result is that, for example, a poll from two weeks ago is penalized more the closer the date is to the election.

##Sample size
Polls are weighted by $sqrt{Sample Size/600}$, where 600 is the most common poll sample size. The effect is polls with smaller sample sizes than normal are penalized more, while polls with larger sample sizes are not considered to benefit as much. WORDING

##Pollster rating
Polls are weighted according to how well the pollster has performed in the past. "How well" the pollster has performed is calculated by taking the average of errors under the pollster's control. [2]

There are three sources of error in a poll: sampling error, temporal error, and methodology errors (what he calls "pollster-induced error", or PIE). Temporal error is incorporated into the model as uncertainty while creating the forecast for the election, based on information available today. Furthermore, the polls he based the ratings on were all taken within two weeks of the election date, and so temporal error can be considered negligible. Approximating sampling error, using the formula $80 * n^{-0.5}$, then allows us to calculate PIE for any poll. 

The PIE of all other polls in the same contest, by a different pollster, are then averaged together to get an "iterated average error" (IAE). For simplicity, I compared the PIE for each poll to all other polls in the same contest, regardless of pollster. The IAE is then subtracted from the PIE to get an idea of how accurate this poll is, compared to the average (called the "+/- rating). A negative +/- rating means the pollster's error is on average lower than other pollsters. 

To turn the +/- rating into a weight for each pollster, the +/- rating is first averaged over all polls conducted by the pollster, "regressed to the mean", then added to the average pollster PIE to get a sense of the absolute value of the error. For simplicity's sake, I did not regress the +/- rating to the mean.

______


#Adjustments to Poll results
##Registered Voters adjustment
Polls are typically either registered voter polls, meaning they sample from all registered voters, or likely voter polls. Likely voter polls tend to be more accurate and, on average, lean 2.7 percentage points more in favor of Republicans than registered voter polls. The results for all registered voter polls are then adjusted 2.7 percentage points towards Republicans.

##Trendline adjustment
In the time between the day a poll was taken and the election date, it is very likely that the political landscape will change (such as one candidate performing extremely well at a debate, thus shifting public opinion towards him). Older polls, therefore, are less relevant today because they come from a different political landscape. The trendline adjustment seeks to account for that.

The polls are split into groups based on the state they were conducted in (national polls are considered part of the "USA" state) and their pollster. A LOESS regression is conducted on each pollster-state subgroup, with the polls' results as the response, and the week as the predictor. The week is defined starting from the current date--the 7 days before today are week 1, the preceding 7 are week 2, and so on. 

This is analagous to a linear regression with model $Result_i =  beta_0 + \beta_1*Pollster_i*State_i + \beta_2*Week_i + \epsilon_i, \epsilon~N(0,\sigma^2)$. 

Some pollster-state subgroups had very little data, and for those subgroups with under 10 data points, I fit a linear regression to the data instead, with model For pollster-state subgroups that had less than 10 polls, I fit a linear regression with model $Result_i =  beta_0 + \beta_1*Week_i + \epsilon_i, \epsilon~N(0,\sigma^2)$.

The result is a regression that represents the average results of a pollster within a state, over time. We use the ______  

##House effects adjustment
Pollsters may have a tendency to produce polls that favor one party over the other, called "house effects." In Nate Silver's model, the "house effect" of a pollster is defined as how much the pollster favors a party, compared to other pollsters (not compared to the election result). 

The house effects of the pollsters are determined using a linear regression. The model is $TAdjusted_i = \beta_0 + \beta_1*State_i + \beta_2*Pollster_i + \epsilon_i, \epsilon~N(0,\sigma^2)$, where TAdjusted is the poll's results, adjusted as described previously, and ______


#Prediction
I took the average of each poll's results for each state, weighted and adjusted as described above. If the average was in favor of Obama, I added the electoral votes for that state to a running total that I creatively named predicted.votes. 

Two states, Maine and Nebraska, split their electoral votes for each party based on their congressional districts. Due to a lack of data, I simply turned the average poll result into a ratio, then multiplied the ratio by the number of electoral votes for the state, to get the number of votes for each candidate.

Polls for Washington D.C. were not included in my dataset, surprisingly, so I used the results from national polls instead, with the idea that they represent the average result, to determine where the electoral votes for D.C. would go.

______



http://fivethirtyeight.com/features/how-fivethirtyeight-calculates-pollster-ratings/
[1] http://fivethirtyeight.com/features/how-the-fivethirtyeight-senate-forecast-model-works/
[2] http://fivethirtyeight.com/features/pollster-ratings-v30/

```{r}
############################
#Read in data
############################
library(stringr)
library(ggplot2)

today = as.Date("2012/10/31")
as.numeric.factor = function(x) {as.numeric(levels(x))[x]}



poll.ratings =  read.csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/pollster-ratings/pollster-ratings.tsv", header=TRUE, sep="\t")
head(poll.ratings)

nat.polls2012 = read.csv("2012_poll_data.csv", header=TRUE, sep="\t")
state.polls2012 = read.csv("2012_poll_data_states.csv", header=TRUE, sep='\t')

nat.polls2012$State = "USA"
polls2012 = rbind(nat.polls2012, state.polls2012)

#format dates
dates = str_split_fixed(as.character(polls2012$Date), " - ", 2)
polls2012$StartDate = as.Date(paste("2012", dates[,1], sep="/")) 
polls2012$EndDate = as.Date(paste("2012", dates[,2], sep="/")) 
polls2012$Date = floor(rowMeans(matrix(c(polls2012$StartDate, polls2012$EndDate), ncol=2)))
polls2012$Date = as.Date(polls2012$Date, origin="1970-01-01")

#format "spread"
polls2012$Spread = as.character(polls2012$Spread)
spread = str_split_fixed(polls2012$Spread, " ", 2)
obama = which(spread[,1] == "Obama")
polls2012$Spread[obama] = as.numeric(spread[obama,2])
polls2012$Spread[-obama] = -as.numeric(spread[-obama,2])
polls2012$Spread[which(is.na(polls2012$Spread))] = 0
polls2012$Spread = as.numeric(polls2012$Spread)

#format sample
polls2012$Sample = as.character(polls2012$Sample)
sample = str_split_fixed(polls2012$Sample, " ", 2)
polls2012$Type = sample[,2]
polls2012$Sample = as.numeric(sample[,1])



head(polls2012)

head(polls)
summary(polls)



#Useful groupings within "polls"
state.polls = which(polls$location != 'US')
polls2000 = which(polls$year == 2000)
polls2004 = which(polls$year == 2004)
polls2008 = which(polls$year == 2008)
#polls2012 = which(polls$year == 2012 & polls$polldate < today & polls$type_simple == 'Pres-G')
partisan = which(polls$partisan != '') #these pollsters should be excluded later
head(state.polls)


############################
#Weighted polling average
############################
#http://fivethirtyeight.com/features/how-fivethirtyeight-calculates-pollster-ratings/#fn-31
#find pollster-induced error

polls.beforetoday = which(polls2012$EndDate < today)
polldata = polls2012[polls.beforetoday,]
polldata$Poll = factor(polldata$Poll, levels=levels(poll.ratings$Pollster))
nax = which(is.na(polldata$Poll))
polldata = polldata[-nax,] #take out polls that haven't been rated by 538


#Calculating recency
#http://fivethirtyeight.com/features/how-the-fivethirtyeight-senate-forecast-model-works/#fn-13
daysfrom = as.numeric(difftime(as.Date("2012/11/01"), today, unit='days'))
recencyrate = log(2)/(14 + 0.2*daysfrom)
timedif = as.numeric(difftime(today, polldata$Date))

polldata$recency = exp(-recencyrate * timedif)


#Weighting by poll rating
polldata$rating = NA
for (i in 1:length(poll.ratings$Polls)){
  pollster = poll.ratings$Pollster[i]
  idx = which(polldata$Poll == pollster)

    polldata$rating[idx] = poll.ratings$Predictive.Plus.Minus[i]
}

polldata$Sample[which(is.na(polldata$Sample))] = 600 #538 policy


#Dataset and description of data can be found at
#https://github.com/fivethirtyeight/data/tree/master/pollster-ratings
polls = read.csv("https://raw.githubusercontent.com/fivethirtyeight/data/master/pollster-ratings/raw-polls.tsv", header=TRUE, sep="\t")
polls = polls[grep(pattern = ".*Pres.*", x=polls$race),] #select only polls for presidential elections
polls$polldate = as.Date(polls$polldate, format='%m/%d/%Y')
polls$electiondate = as.Date(polls$electiondate, format='%m/%d/%Y')

###################
#This was not included in the final predictions because  turns out that 
#the historical polling data I was using did not include all of the pollsters
#in the 2012 election. (I ended up using 538's pollster ratings on GitHub). 
#But I spent some time on it so I wanted to include it in the code to hand in.
##

polls$sample.error = 80 * polls$samplesize^-0.5
polls$PIE = polls$error - polls$sample.error
polls$IAE = NA 
polls$plus.min = NA 
n = length(polls$PIE)
for (i in 1:n){
  race = polls$race[i]
  others = polls[polls$race == race,]
  same = which(others$pollster == polls$pollster[i])
  others = others[-same,]
  polls$IAE[i] = ifelse(length(others$pollno != 0), mean(others$PIE), 0)
  polls$plus.min[i] = polls$PIE[i] - polls$IAE[i]
} 



se = sd(polls$IAE)/sqrt(length(polls$IAE)) 
pollsters = data.frame(pollster = levels(factor(polls$pollster)), PIE=NA, IAE=NA, plus.min = NA)
for (i in 1:length(pollsters$pollster)){
  pster = as.character(pollsters$pollster[i])
  pollsters$PIE[i] = mean(polls$PIE[polls$pollster == pster])
  pollsters$IAE[i] = mean(polls$IAE[polls$pollster == pster])
  
  pollsters$plus.min[i] = mean(polls$plus.min[polls$pollster == pster])
  
}
#this is basically comparison to methodologically perfect poll
pollsters$plus.min = pollsters$plus.min + 1.49

###################


for (i in 1:length(polldata$Poll)) {
  pster = as.character(polldata$Poll[i])
  tot.error = poll.ratings$Simple.Plus.Minus[poll.ratings$Pollster == pster] + 1.49 + 80*polldata$Sample[i]^-0.5
  ESS = 6400 * (tot.error^-2) #effective sample size
  polldata$rating[i] = ESS/283
}


#statepollsters
#going backwards in time
#calculate cumulative effective sample size (most recent, then most recent + second most, etc)
#difference b/w CESS is the marginal and treated as actual sample size

#Weighting by sample weight
polldata$Sampleweight = sqrt(polldata$Sample/600)
#http://fivethirtyeight.com/features/polls-now-weighted-by-sample-size/


##########################
##Likely Voter adjustment
##########################

rv = which(polldata$Type == "RV")
polldata$spread[rv] = polldata$spread[rv] - 2.7
#registered voter polls tend to differ from likely voter polls by 2.7 percentage points




##########################
##Trend Line Adjustment
##########################
#Create "week" variable
#Week is defined using "today"
polldata$Week = today - 7*floor(difftime(today, polldata$Date, unit='days')/7)

polldata$week = as.numeric(floor(difftime(today, polldata$Date, unit='days')/7))

polldata$StatePollster = paste(polldata$State,polldata$Poll)


statepollsters = data.frame(StatePollsters = as.character(levels(factor(polldata$StatePollster))))

polldata$TAdjustment = NA
trend.fits = lapply(statepollsters$StatePollsters, function(x){
  data = polldata[polldata$StatePollster == x,]
  fit = NA
  if (length(data$Poll) < 5) {
    0
  }
#  if (length(data$Poll) < 10){ #apparently changing this to ifelse breaks lapply
#    fit = lm(Spread ~ week, data=data)
#    predict(fit, newdata=data.frame(week = 0)) - predict(fit, newdata=data)
#  }
  else {
    fit = loess(Spread ~ week, data=data, span=0.85, model=TRUE, surface='direct')
    predict(fit, newdata=data.frame(week = 1)) - predict(fit, newdata=data)
  }
})


for (i in 1:length(statepollsters$StatePollsters)) {
  polldata$TAdjustment[polldata$StatePollster == statepollsters$StatePollsters[i]] = trend.fits[[i]] #apparently doing this inside lapply breaks R
}

polldata$TAdjusted = polldata$TAdjustment + polldata$Spread
max(polldata$TAdjusted)

#################
##House effects adjustment
#################

#remove polls with only one poll per state 
library(reshape2)
s = state.abb
s[51] = "USA"

count.states = dcast(polldata, State + Poll ~., length) 
#WV, TX, SD, OK, MS, MD, HI, LA, KY, IL, HI, AR
#OK Soonerpoll.com, LA Clarus, AR Hendrix


count.polls = dcast(polldata, Poll  ~., length)
singleton.idx = which(count.polls$. == 1) #states with polls conducted by only one pollsters
a = count.polls$Poll[singleton.idx]

a.idx = sapply(a, function(x){
  which(polldata$Poll == x)
})

ok.idx = which(polldata$State == "OK" )
la.idx = which(polldata$State == "LA" )
ar.idx = which(polldata$State == "AR" )

housefx.fit = lm(TAdjusted ~ State + Poll, data=polldata[-c(ok.idx, la.idx, ar.idx),])

library(car)
vif(housefx.fit)
res = housefx.fit$residuals

crPlots(housefx.fit) 

outlierTest(housefx.fit) #744, 748, 746, 743, 745
plot(housefx.fit$fitted.values ~ res)

cd=cooks.distance(fit)
plot(cd)


qqnorm(res)
qqline(res)


#removing outliers
housefx.fit2 = lm(TAdjusted ~ Poll + State, data=polldata[-out.idx,])
res = housefx.fit2$residuals
r.star = rstudent(housefx.fit2)
vif(housefx.fit2)

crPlots(housefx.fit2) 

outlierTest(housefx.fit2)
plot(housefx.fit2$fitted.values ~ r.star)
abline(0,0,col="red")

qqnorm(res)
qqline(res)

housefx = summary(housefx.fit)
hfx = housefx$coefficients[-seq(1,42,1),1]
se = housefx$coefficients[-seq(1,42,1),2]

n = nrow(polldata[-c(ok.idx, la.idx, ar.idx),])
p = rankMatrix(X)
p = p[1]

buffers = 2*se * qt(0.95, n - p)
housefx = data.frame(Poll = names(hfx), HouseEffect = as.numeric(hfx), Buffer = as.numeric(se))
names = str_split_fixed(housefx$Poll, "Poll", 2)
housefx$Poll = names[,2]

housefx$Total = ifelse(housefx$HouseEffect < 0, housefx$HouseEffect + housefx$Buffer, housefx$HouseEffect - housefx$Buffer)

polldata$HXAdjusted = polldata$TAdjusted
for (i in 1:length(polldata$Poll)){
  pster = as.character(polldata$Poll[i])
  total.hfx = housefx$Total[housefx$Poll == pster]
  if (length(total.hfx != 0)) {
    if (pster == "Public Policy Polling") {
    #length of total.hfx will be 0 if it was a poll conducted by a pollster
    #who only has polls in one of the states that were taken out
    print(total.hfx)}
    polldata$HXAdjusted[i] = polldata$TAdjusted[i] - total.hfx
  }
}



#use coefficients of housefx as housefx
#depending on sign of housefx:
#   - neg housefx: add buffer, subtract the sum from TAdjusted
#   - pos housefx: subtract buffer, subtract the sum from TAdjusted (I think)



#################
##2012 predictions
################

#http://fivethirtyeight.com/features/how-the-fivethirtyeight-senate-forecast-model-works/#fn-13
#multiply final adjusted spread by all weights and then average over each state
#in Maine and Nebraska, average over congressional district?


elect.votes = read.csv("electoral_votes.csv")
elect.votes$State = as.character(elect.votes$State)
elect.votes$StateAbb = NA
for (i in 1:51){
  idx= which(state.name == elect.votes$State[i])
  if (length(idx) != 0){elect.votes$StateAbb[i] = state.abb[idx]}
  else {elect.votes$StateAbb[i] = "DC"}
}
head(elect.votes) #number of votes for each state

predicted.votes = data.frame(State = elect.votes$StateAbb, Votes = 0, region=tolower(elect.votes$State))
head(predicted.votes)

for (i in 1:51){
  if (elect.votes$StateAbb[i] == 'DC') {
    nationaldata = polldata[polldata$State == 'USA',]
    spread = nationaldata$HXAdjusted
    pop.vote = mean(spread * nationaldata$Sampleweight * nationaldata$recency * nationaldata$rating)
    predicted.votes$Votes[i] = ifelse(pop.vote > 0, elect.votes$Votes[i], 0)
    
  }
  else {
    statedata = polldata[polldata$State == elect.votes$StateAbb[i],]
    spread = statedata$HXAdjusted 
    pop.vote = mean(spread * statedata$Sampleweight * statedata$recency * statedata$rating)
    if (elect.votes$StateAbb[i] == 'ME' | elect.votes$StateAbb[i] == 'NE'){
      obama.perc = pop.vote + 50
      predicted.votes$Votes[i] = round(obama.perc/100 * elect.votes$Votes[i])
    }
    else {
      predicted.votes$Votes[i] = ifelse(pop.vote > 0, elect.votes$Votes[i], 0)
    }
  }
}

predicted.votes

library(googleVis)

all_states = map_data("state")
predicted.votes$Obama = ifelse(predicted.votes$Votes > 0, 0, 1)
predicted.votes$Obama = ifelse(is.na(predicted.votes$Votes), 2, predicted.votes$Obama)
#Total = merge(all_states, predicted.votes, by="region")
#head(Total)
#Total = Total[Total$region!="district of columbia",]


map = gvisGeoMap(data.frame(state = predicted.votes$region, outcome = predicted.votes$Obama), locationvar='state', numvar='outcome', options=list(region='US',dataMode='regions',colors="['0x3B5295','0x8A2448', '0xFFFFFF']", showLegend = FALSE))
plot(map)

elect.votes$actual = c(0,0,0,0,55,9,7,3,3,29,0,4,0,20,0,6,0,0,0,4,10,11,16,10,0,0,0,0,6,4,14,5,29,0,0,18,0,7,20,4,0,0,0,0,0,3,13,12,0,10,0)
elect.votes$Obama = ifelse(elect.votes$actual > 0, 0, 1)
elect.votes$Obama = ifelse(is.na(elect.votes$actual), 2, elect.votes$Obama)

actual.map = gvisGeoMap(data.frame(state = elect.votes$State, outcome = elect.votes$Obama), locationvar='state', numvar='outcome', options=list(region='US',dataMode='regions',colors="['0x3B5295','0x8A2448']", showLegend = FALSE))
plot(actual.map)

predicted.votes$actual = elect.votes$actual

predicted.votes$error = predicted.votes$Votes - predicted.votes$actual != 0

err.idx = which(predicted.votes$error)
predicted.votes$State[err.idx]

polldata[polldata$State == 'MO',]
#AZ, ME, MO, NE NH, NC
```